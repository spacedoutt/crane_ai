{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 1.9929328621908127,
  "eval_steps": 500,
  "global_step": 282,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007067137809187279,
      "grad_norm": 2.022409439086914,
      "learning_rate": 6.666666666666667e-06,
      "loss": 2.4494,
      "step": 1
    },
    {
      "epoch": 0.014134275618374558,
      "grad_norm": 0.9793543219566345,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 2.5234,
      "step": 2
    },
    {
      "epoch": 0.02120141342756184,
      "grad_norm": 1.6935573816299438,
      "learning_rate": 2e-05,
      "loss": 3.0289,
      "step": 3
    },
    {
      "epoch": 0.028268551236749116,
      "grad_norm": 1.7486528158187866,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 3.0376,
      "step": 4
    },
    {
      "epoch": 0.0353356890459364,
      "grad_norm": 1.5069012641906738,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 2.8815,
      "step": 5
    },
    {
      "epoch": 0.04240282685512368,
      "grad_norm": 2.1140050888061523,
      "learning_rate": 4e-05,
      "loss": 3.136,
      "step": 6
    },
    {
      "epoch": 0.04946996466431095,
      "grad_norm": 1.7252628803253174,
      "learning_rate": 4.666666666666667e-05,
      "loss": 3.1061,
      "step": 7
    },
    {
      "epoch": 0.05653710247349823,
      "grad_norm": 2.205775260925293,
      "learning_rate": 5.333333333333333e-05,
      "loss": 3.0103,
      "step": 8
    },
    {
      "epoch": 0.0636042402826855,
      "grad_norm": 2.2606282234191895,
      "learning_rate": 6e-05,
      "loss": 3.185,
      "step": 9
    },
    {
      "epoch": 0.0706713780918728,
      "grad_norm": 1.8141779899597168,
      "learning_rate": 6.666666666666667e-05,
      "loss": 2.7712,
      "step": 10
    },
    {
      "epoch": 0.07773851590106007,
      "grad_norm": 2.1690306663513184,
      "learning_rate": 7.333333333333333e-05,
      "loss": 3.3446,
      "step": 11
    },
    {
      "epoch": 0.08480565371024736,
      "grad_norm": 1.9139608144760132,
      "learning_rate": 8e-05,
      "loss": 2.8999,
      "step": 12
    },
    {
      "epoch": 0.09187279151943463,
      "grad_norm": 1.8831859827041626,
      "learning_rate": 8.666666666666667e-05,
      "loss": 2.8619,
      "step": 13
    },
    {
      "epoch": 0.0989399293286219,
      "grad_norm": 1.8665746450424194,
      "learning_rate": 9.333333333333334e-05,
      "loss": 2.8851,
      "step": 14
    },
    {
      "epoch": 0.10600706713780919,
      "grad_norm": 2.231839656829834,
      "learning_rate": 0.0001,
      "loss": 3.11,
      "step": 15
    },
    {
      "epoch": 0.11307420494699646,
      "grad_norm": 2.2183234691619873,
      "learning_rate": 9.99965389153533e-05,
      "loss": 3.059,
      "step": 16
    },
    {
      "epoch": 0.12014134275618374,
      "grad_norm": 1.883208155632019,
      "learning_rate": 9.998615614057742e-05,
      "loss": 2.544,
      "step": 17
    },
    {
      "epoch": 0.127208480565371,
      "grad_norm": 1.95951247215271,
      "learning_rate": 9.996885311309891e-05,
      "loss": 2.7836,
      "step": 18
    },
    {
      "epoch": 0.13427561837455831,
      "grad_norm": 1.8068021535873413,
      "learning_rate": 9.994463222840746e-05,
      "loss": 2.6707,
      "step": 19
    },
    {
      "epoch": 0.1413427561837456,
      "grad_norm": 1.9451733827590942,
      "learning_rate": 9.991349683972434e-05,
      "loss": 2.9288,
      "step": 20
    },
    {
      "epoch": 0.14840989399293286,
      "grad_norm": 1.9017914533615112,
      "learning_rate": 9.987545125753819e-05,
      "loss": 2.3736,
      "step": 21
    },
    {
      "epoch": 0.15547703180212014,
      "grad_norm": 1.8215432167053223,
      "learning_rate": 9.983050074900824e-05,
      "loss": 2.7856,
      "step": 22
    },
    {
      "epoch": 0.1625441696113074,
      "grad_norm": 1.870759129524231,
      "learning_rate": 9.977865153723507e-05,
      "loss": 2.7586,
      "step": 23
    },
    {
      "epoch": 0.1696113074204947,
      "grad_norm": 2.056816577911377,
      "learning_rate": 9.97199108003991e-05,
      "loss": 2.8988,
      "step": 24
    },
    {
      "epoch": 0.17667844522968199,
      "grad_norm": 2.137852668762207,
      "learning_rate": 9.965428667076686e-05,
      "loss": 3.3246,
      "step": 25
    },
    {
      "epoch": 0.18374558303886926,
      "grad_norm": 1.8751949071884155,
      "learning_rate": 9.958178823356503e-05,
      "loss": 2.5371,
      "step": 26
    },
    {
      "epoch": 0.19081272084805653,
      "grad_norm": 2.405423879623413,
      "learning_rate": 9.950242552572271e-05,
      "loss": 2.7347,
      "step": 27
    },
    {
      "epoch": 0.1978798586572438,
      "grad_norm": 3.065908670425415,
      "learning_rate": 9.941620953448194e-05,
      "loss": 2.9675,
      "step": 28
    },
    {
      "epoch": 0.2049469964664311,
      "grad_norm": 2.038029909133911,
      "learning_rate": 9.93231521958764e-05,
      "loss": 2.6089,
      "step": 29
    },
    {
      "epoch": 0.21201413427561838,
      "grad_norm": 1.9041630029678345,
      "learning_rate": 9.922326639307917e-05,
      "loss": 3.1188,
      "step": 30
    },
    {
      "epoch": 0.21908127208480566,
      "grad_norm": 2.1505870819091797,
      "learning_rate": 9.911656595461898e-05,
      "loss": 2.7294,
      "step": 31
    },
    {
      "epoch": 0.22614840989399293,
      "grad_norm": 2.1394741535186768,
      "learning_rate": 9.900306565246578e-05,
      "loss": 2.7965,
      "step": 32
    },
    {
      "epoch": 0.2332155477031802,
      "grad_norm": 2.3517096042633057,
      "learning_rate": 9.888278119998573e-05,
      "loss": 2.7225,
      "step": 33
    },
    {
      "epoch": 0.24028268551236748,
      "grad_norm": 2.5036532878875732,
      "learning_rate": 9.875572924976568e-05,
      "loss": 3.004,
      "step": 34
    },
    {
      "epoch": 0.24734982332155478,
      "grad_norm": 4.801952362060547,
      "learning_rate": 9.86219273913078e-05,
      "loss": 3.9754,
      "step": 35
    },
    {
      "epoch": 0.254416961130742,
      "grad_norm": 1.7934314012527466,
      "learning_rate": 9.848139414859441e-05,
      "loss": 2.1512,
      "step": 36
    },
    {
      "epoch": 0.26148409893992935,
      "grad_norm": 1.6315264701843262,
      "learning_rate": 9.833414897752347e-05,
      "loss": 2.171,
      "step": 37
    },
    {
      "epoch": 0.26855123674911663,
      "grad_norm": 1.4425857067108154,
      "learning_rate": 9.8180212263215e-05,
      "loss": 2.1051,
      "step": 38
    },
    {
      "epoch": 0.2756183745583039,
      "grad_norm": 1.155803918838501,
      "learning_rate": 9.801960531718896e-05,
      "loss": 2.4763,
      "step": 39
    },
    {
      "epoch": 0.2826855123674912,
      "grad_norm": 1.0320155620574951,
      "learning_rate": 9.785235037441474e-05,
      "loss": 2.199,
      "step": 40
    },
    {
      "epoch": 0.28975265017667845,
      "grad_norm": 0.9723770618438721,
      "learning_rate": 9.767847059023291e-05,
      "loss": 2.2018,
      "step": 41
    },
    {
      "epoch": 0.2968197879858657,
      "grad_norm": 1.0566339492797852,
      "learning_rate": 9.749799003714954e-05,
      "loss": 2.2219,
      "step": 42
    },
    {
      "epoch": 0.303886925795053,
      "grad_norm": 1.1914960145950317,
      "learning_rate": 9.731093370150349e-05,
      "loss": 2.0384,
      "step": 43
    },
    {
      "epoch": 0.31095406360424027,
      "grad_norm": 1.0611823797225952,
      "learning_rate": 9.71173274800072e-05,
      "loss": 2.4735,
      "step": 44
    },
    {
      "epoch": 0.31802120141342755,
      "grad_norm": 0.922407865524292,
      "learning_rate": 9.691719817616147e-05,
      "loss": 2.3903,
      "step": 45
    },
    {
      "epoch": 0.3250883392226148,
      "grad_norm": 0.9721489548683167,
      "learning_rate": 9.67105734965448e-05,
      "loss": 2.3799,
      "step": 46
    },
    {
      "epoch": 0.3321554770318021,
      "grad_norm": 0.9984425902366638,
      "learning_rate": 9.64974820469774e-05,
      "loss": 2.2864,
      "step": 47
    },
    {
      "epoch": 0.3392226148409894,
      "grad_norm": 1.0947078466415405,
      "learning_rate": 9.627795332856107e-05,
      "loss": 2.6168,
      "step": 48
    },
    {
      "epoch": 0.3462897526501767,
      "grad_norm": 1.1075009107589722,
      "learning_rate": 9.605201773359485e-05,
      "loss": 2.3285,
      "step": 49
    },
    {
      "epoch": 0.35335689045936397,
      "grad_norm": 1.056508183479309,
      "learning_rate": 9.581970654136751e-05,
      "loss": 1.9736,
      "step": 50
    },
    {
      "epoch": 0.36042402826855124,
      "grad_norm": 1.2873791456222534,
      "learning_rate": 9.55810519138271e-05,
      "loss": 2.6025,
      "step": 51
    },
    {
      "epoch": 0.3674911660777385,
      "grad_norm": 1.0405802726745605,
      "learning_rate": 9.533608689112827e-05,
      "loss": 2.0896,
      "step": 52
    },
    {
      "epoch": 0.3745583038869258,
      "grad_norm": 1.2283798456192017,
      "learning_rate": 9.508484538705824e-05,
      "loss": 2.4158,
      "step": 53
    },
    {
      "epoch": 0.38162544169611307,
      "grad_norm": 1.3034777641296387,
      "learning_rate": 9.482736218434143e-05,
      "loss": 2.6009,
      "step": 54
    },
    {
      "epoch": 0.38869257950530034,
      "grad_norm": 1.1551556587219238,
      "learning_rate": 9.456367292982429e-05,
      "loss": 2.3453,
      "step": 55
    },
    {
      "epoch": 0.3957597173144876,
      "grad_norm": 1.1859220266342163,
      "learning_rate": 9.429381412953999e-05,
      "loss": 2.2731,
      "step": 56
    },
    {
      "epoch": 0.4028268551236749,
      "grad_norm": 1.333423137664795,
      "learning_rate": 9.401782314365457e-05,
      "loss": 2.3263,
      "step": 57
    },
    {
      "epoch": 0.4098939929328622,
      "grad_norm": 1.5633811950683594,
      "learning_rate": 9.373573818129458e-05,
      "loss": 2.5062,
      "step": 58
    },
    {
      "epoch": 0.4169611307420495,
      "grad_norm": 1.2807029485702515,
      "learning_rate": 9.344759829525733e-05,
      "loss": 2.5807,
      "step": 59
    },
    {
      "epoch": 0.42402826855123676,
      "grad_norm": 1.6720954179763794,
      "learning_rate": 9.315344337660421e-05,
      "loss": 2.8877,
      "step": 60
    },
    {
      "epoch": 0.43109540636042404,
      "grad_norm": 1.7105133533477783,
      "learning_rate": 9.285331414913815e-05,
      "loss": 2.9115,
      "step": 61
    },
    {
      "epoch": 0.4381625441696113,
      "grad_norm": 1.6162095069885254,
      "learning_rate": 9.254725216376561e-05,
      "loss": 2.6416,
      "step": 62
    },
    {
      "epoch": 0.4452296819787986,
      "grad_norm": 1.718125820159912,
      "learning_rate": 9.22352997927441e-05,
      "loss": 3.2215,
      "step": 63
    },
    {
      "epoch": 0.45229681978798586,
      "grad_norm": 1.78681480884552,
      "learning_rate": 9.191750022381614e-05,
      "loss": 3.0858,
      "step": 64
    },
    {
      "epoch": 0.45936395759717313,
      "grad_norm": 1.4100565910339355,
      "learning_rate": 9.159389745423002e-05,
      "loss": 2.9017,
      "step": 65
    },
    {
      "epoch": 0.4664310954063604,
      "grad_norm": 1.4898382425308228,
      "learning_rate": 9.126453628464888e-05,
      "loss": 2.6049,
      "step": 66
    },
    {
      "epoch": 0.4734982332155477,
      "grad_norm": 1.5636359453201294,
      "learning_rate": 9.092946231294819e-05,
      "loss": 2.7736,
      "step": 67
    },
    {
      "epoch": 0.48056537102473496,
      "grad_norm": 1.6875892877578735,
      "learning_rate": 9.058872192790313e-05,
      "loss": 2.6217,
      "step": 68
    },
    {
      "epoch": 0.4876325088339223,
      "grad_norm": 2.276315450668335,
      "learning_rate": 9.024236230276629e-05,
      "loss": 3.3577,
      "step": 69
    },
    {
      "epoch": 0.49469964664310956,
      "grad_norm": 3.0389819145202637,
      "learning_rate": 8.98904313887369e-05,
      "loss": 2.9188,
      "step": 70
    },
    {
      "epoch": 0.5017667844522968,
      "grad_norm": 0.970057487487793,
      "learning_rate": 8.953297790832231e-05,
      "loss": 1.8327,
      "step": 71
    },
    {
      "epoch": 0.508833922261484,
      "grad_norm": 1.0968877077102661,
      "learning_rate": 8.917005134859263e-05,
      "loss": 1.8808,
      "step": 72
    },
    {
      "epoch": 0.5159010600706714,
      "grad_norm": 1.0374150276184082,
      "learning_rate": 8.88017019543296e-05,
      "loss": 2.1717,
      "step": 73
    },
    {
      "epoch": 0.5229681978798587,
      "grad_norm": 1.0458382368087769,
      "learning_rate": 8.842798072107054e-05,
      "loss": 2.0629,
      "step": 74
    },
    {
      "epoch": 0.5300353356890459,
      "grad_norm": 1.0578193664550781,
      "learning_rate": 8.80489393880484e-05,
      "loss": 2.4489,
      "step": 75
    },
    {
      "epoch": 0.5371024734982333,
      "grad_norm": 1.0196895599365234,
      "learning_rate": 8.766463043102864e-05,
      "loss": 1.9512,
      "step": 76
    },
    {
      "epoch": 0.5441696113074205,
      "grad_norm": 0.9518066644668579,
      "learning_rate": 8.727510705504454e-05,
      "loss": 1.8802,
      "step": 77
    },
    {
      "epoch": 0.5512367491166078,
      "grad_norm": 0.8494212627410889,
      "learning_rate": 8.688042318703111e-05,
      "loss": 2.2707,
      "step": 78
    },
    {
      "epoch": 0.558303886925795,
      "grad_norm": 0.9143539071083069,
      "learning_rate": 8.648063346835942e-05,
      "loss": 2.0863,
      "step": 79
    },
    {
      "epoch": 0.5653710247349824,
      "grad_norm": 0.9092293381690979,
      "learning_rate": 8.607579324727175e-05,
      "loss": 2.0571,
      "step": 80
    },
    {
      "epoch": 0.5724381625441696,
      "grad_norm": 0.9530013203620911,
      "learning_rate": 8.566595857121902e-05,
      "loss": 2.3529,
      "step": 81
    },
    {
      "epoch": 0.5795053003533569,
      "grad_norm": 0.9184759855270386,
      "learning_rate": 8.525118617910143e-05,
      "loss": 1.9959,
      "step": 82
    },
    {
      "epoch": 0.5865724381625441,
      "grad_norm": 0.9186165928840637,
      "learning_rate": 8.483153349341335e-05,
      "loss": 2.0539,
      "step": 83
    },
    {
      "epoch": 0.5936395759717314,
      "grad_norm": 1.0846062898635864,
      "learning_rate": 8.440705861229344e-05,
      "loss": 1.8918,
      "step": 84
    },
    {
      "epoch": 0.6007067137809188,
      "grad_norm": 1.2125283479690552,
      "learning_rate": 8.397782030148147e-05,
      "loss": 2.1233,
      "step": 85
    },
    {
      "epoch": 0.607773851590106,
      "grad_norm": 1.1552107334136963,
      "learning_rate": 8.354387798618253e-05,
      "loss": 2.6566,
      "step": 86
    },
    {
      "epoch": 0.6148409893992933,
      "grad_norm": 1.175605058670044,
      "learning_rate": 8.310529174284004e-05,
      "loss": 2.4801,
      "step": 87
    },
    {
      "epoch": 0.6219081272084805,
      "grad_norm": 1.2986605167388916,
      "learning_rate": 8.266212229081847e-05,
      "loss": 2.5867,
      "step": 88
    },
    {
      "epoch": 0.6289752650176679,
      "grad_norm": 1.364498496055603,
      "learning_rate": 8.221443098399732e-05,
      "loss": 2.7839,
      "step": 89
    },
    {
      "epoch": 0.6360424028268551,
      "grad_norm": 1.1776494979858398,
      "learning_rate": 8.176227980227694e-05,
      "loss": 2.0666,
      "step": 90
    },
    {
      "epoch": 0.6431095406360424,
      "grad_norm": 1.3584233522415161,
      "learning_rate": 8.130573134299782e-05,
      "loss": 2.5463,
      "step": 91
    },
    {
      "epoch": 0.6501766784452296,
      "grad_norm": 1.1593364477157593,
      "learning_rate": 8.084484881227448e-05,
      "loss": 2.3973,
      "step": 92
    },
    {
      "epoch": 0.657243816254417,
      "grad_norm": 1.3711793422698975,
      "learning_rate": 8.037969601624495e-05,
      "loss": 2.5453,
      "step": 93
    },
    {
      "epoch": 0.6643109540636042,
      "grad_norm": 1.0680524110794067,
      "learning_rate": 7.991033735223729e-05,
      "loss": 2.3226,
      "step": 94
    },
    {
      "epoch": 0.6713780918727915,
      "grad_norm": 1.5485703945159912,
      "learning_rate": 7.943683779985413e-05,
      "loss": 2.4686,
      "step": 95
    },
    {
      "epoch": 0.6784452296819788,
      "grad_norm": 1.2072105407714844,
      "learning_rate": 7.895926291197667e-05,
      "loss": 2.239,
      "step": 96
    },
    {
      "epoch": 0.6855123674911661,
      "grad_norm": 1.3206794261932373,
      "learning_rate": 7.847767880568945e-05,
      "loss": 2.6581,
      "step": 97
    },
    {
      "epoch": 0.6925795053003534,
      "grad_norm": 1.349073052406311,
      "learning_rate": 7.799215215312667e-05,
      "loss": 2.5845,
      "step": 98
    },
    {
      "epoch": 0.6996466431095406,
      "grad_norm": 1.5286486148834229,
      "learning_rate": 7.750275017224207e-05,
      "loss": 2.6259,
      "step": 99
    },
    {
      "epoch": 0.7067137809187279,
      "grad_norm": 1.5309984683990479,
      "learning_rate": 7.700954061750293e-05,
      "loss": 2.4393,
      "step": 100
    },
    {
      "epoch": 0.7137809187279152,
      "grad_norm": 1.7677701711654663,
      "learning_rate": 7.651259177050996e-05,
      "loss": 2.6965,
      "step": 101
    },
    {
      "epoch": 0.7208480565371025,
      "grad_norm": NaN,
      "learning_rate": 7.651259177050996e-05,
      "loss": 2.96,
      "step": 102
    },
    {
      "epoch": 0.7279151943462897,
      "grad_norm": 2.2894234657287598,
      "learning_rate": 7.60119724305441e-05,
      "loss": 3.1429,
      "step": 103
    },
    {
      "epoch": 0.734982332155477,
      "grad_norm": 2.8899526596069336,
      "learning_rate": 7.550775190504189e-05,
      "loss": 3.1791,
      "step": 104
    },
    {
      "epoch": 0.7420494699646644,
      "grad_norm": 4.414155960083008,
      "learning_rate": 7.500000000000001e-05,
      "loss": 3.2027,
      "step": 105
    },
    {
      "epoch": 0.7491166077738516,
      "grad_norm": 0.9304723739624023,
      "learning_rate": 7.448878701031142e-05,
      "loss": 1.8738,
      "step": 106
    },
    {
      "epoch": 0.7561837455830389,
      "grad_norm": 0.773618221282959,
      "learning_rate": 7.397418371003333e-05,
      "loss": 1.7726,
      "step": 107
    },
    {
      "epoch": 0.7632508833922261,
      "grad_norm": 0.8355372548103333,
      "learning_rate": 7.345626134258898e-05,
      "loss": 1.8941,
      "step": 108
    },
    {
      "epoch": 0.7703180212014135,
      "grad_norm": 0.8948445320129395,
      "learning_rate": 7.293509161090452e-05,
      "loss": 2.0912,
      "step": 109
    },
    {
      "epoch": 0.7773851590106007,
      "grad_norm": 0.8098867535591125,
      "learning_rate": 7.241074666748227e-05,
      "loss": 2.2609,
      "step": 110
    },
    {
      "epoch": 0.784452296819788,
      "grad_norm": 0.8708932399749756,
      "learning_rate": 7.188329910441154e-05,
      "loss": 2.354,
      "step": 111
    },
    {
      "epoch": 0.7915194346289752,
      "grad_norm": 1.0046172142028809,
      "learning_rate": 7.13528219433188e-05,
      "loss": 2.2907,
      "step": 112
    },
    {
      "epoch": 0.7985865724381626,
      "grad_norm": 0.8849496841430664,
      "learning_rate": 7.081938862525839e-05,
      "loss": 2.0898,
      "step": 113
    },
    {
      "epoch": 0.8056537102473498,
      "grad_norm": 1.120928168296814,
      "learning_rate": 7.028307300054499e-05,
      "loss": 2.322,
      "step": 114
    },
    {
      "epoch": 0.8127208480565371,
      "grad_norm": 0.93724524974823,
      "learning_rate": 6.974394931852956e-05,
      "loss": 2.5157,
      "step": 115
    },
    {
      "epoch": 0.8197879858657244,
      "grad_norm": 1.1139490604400635,
      "learning_rate": 6.920209221732006e-05,
      "loss": 2.3832,
      "step": 116
    },
    {
      "epoch": 0.8268551236749117,
      "grad_norm": 0.9670174717903137,
      "learning_rate": 6.865757671344827e-05,
      "loss": 1.9223,
      "step": 117
    },
    {
      "epoch": 0.833922261484099,
      "grad_norm": 1.027444839477539,
      "learning_rate": 6.811047819148413e-05,
      "loss": 2.3848,
      "step": 118
    },
    {
      "epoch": 0.8409893992932862,
      "grad_norm": 1.0481902360916138,
      "learning_rate": 6.756087239359947e-05,
      "loss": 2.3188,
      "step": 119
    },
    {
      "epoch": 0.8480565371024735,
      "grad_norm": 1.1003977060317993,
      "learning_rate": 6.700883540908184e-05,
      "loss": 2.6448,
      "step": 120
    },
    {
      "epoch": 0.8551236749116607,
      "grad_norm": 1.1545801162719727,
      "learning_rate": 6.64544436638005e-05,
      "loss": 1.7246,
      "step": 121
    },
    {
      "epoch": 0.8621908127208481,
      "grad_norm": 1.1439719200134277,
      "learning_rate": 6.589777390962575e-05,
      "loss": 2.4838,
      "step": 122
    },
    {
      "epoch": 0.8692579505300353,
      "grad_norm": 1.130408525466919,
      "learning_rate": 6.533890321380319e-05,
      "loss": 2.242,
      "step": 123
    },
    {
      "epoch": 0.8763250883392226,
      "grad_norm": 1.1255364418029785,
      "learning_rate": 6.477790894828421e-05,
      "loss": 2.6014,
      "step": 124
    },
    {
      "epoch": 0.8833922261484098,
      "grad_norm": 1.1765817403793335,
      "learning_rate": 6.421486877901437e-05,
      "loss": 2.4872,
      "step": 125
    },
    {
      "epoch": 0.8904593639575972,
      "grad_norm": 1.3128888607025146,
      "learning_rate": 6.364986065518106e-05,
      "loss": 2.2926,
      "step": 126
    },
    {
      "epoch": 0.8975265017667845,
      "grad_norm": 1.3941888809204102,
      "learning_rate": 6.308296279842205e-05,
      "loss": 3.008,
      "step": 127
    },
    {
      "epoch": 0.9045936395759717,
      "grad_norm": 1.3177193403244019,
      "learning_rate": 6.251425369199599e-05,
      "loss": 2.2648,
      "step": 128
    },
    {
      "epoch": 0.911660777385159,
      "grad_norm": 1.3153578042984009,
      "learning_rate": 6.194381206991722e-05,
      "loss": 2.3924,
      "step": 129
    },
    {
      "epoch": 0.9187279151943463,
      "grad_norm": 1.636897087097168,
      "learning_rate": 6.137171690605533e-05,
      "loss": 2.6597,
      "step": 130
    },
    {
      "epoch": 0.9257950530035336,
      "grad_norm": 1.5154238939285278,
      "learning_rate": 6.079804740320181e-05,
      "loss": 2.5498,
      "step": 131
    },
    {
      "epoch": 0.9328621908127208,
      "grad_norm": 1.5087813138961792,
      "learning_rate": 6.022288298210501e-05,
      "loss": 2.8785,
      "step": 132
    },
    {
      "epoch": 0.9399293286219081,
      "grad_norm": 1.4068495035171509,
      "learning_rate": 5.9646303270474845e-05,
      "loss": 2.5725,
      "step": 133
    },
    {
      "epoch": 0.9469964664310954,
      "grad_norm": 1.8009872436523438,
      "learning_rate": 5.9068388091958795e-05,
      "loss": 2.9385,
      "step": 134
    },
    {
      "epoch": 0.9540636042402827,
      "grad_norm": 1.6855831146240234,
      "learning_rate": 5.848921745509094e-05,
      "loss": 2.9886,
      "step": 135
    },
    {
      "epoch": 0.9611307420494699,
      "grad_norm": 1.928753137588501,
      "learning_rate": 5.79088715422152e-05,
      "loss": 3.0619,
      "step": 136
    },
    {
      "epoch": 0.9681978798586572,
      "grad_norm": 1.7998298406600952,
      "learning_rate": 5.7327430698384775e-05,
      "loss": 2.5613,
      "step": 137
    },
    {
      "epoch": 0.9752650176678446,
      "grad_norm": 2.052446126937866,
      "learning_rate": 5.6744975420238745e-05,
      "loss": 2.5973,
      "step": 138
    },
    {
      "epoch": 0.9823321554770318,
      "grad_norm": 2.038006544113159,
      "learning_rate": 5.616158634485793e-05,
      "loss": 2.7022,
      "step": 139
    },
    {
      "epoch": 0.9893992932862191,
      "grad_norm": 3.475146532058716,
      "learning_rate": 5.557734423860123e-05,
      "loss": 3.3906,
      "step": 140
    },
    {
      "epoch": 0.9964664310954063,
      "grad_norm": 1.2627288103103638,
      "learning_rate": 5.499232998592399e-05,
      "loss": 2.2368,
      "step": 141
    },
    {
      "epoch": 1.0035335689045937,
      "grad_norm": 1.0439223051071167,
      "learning_rate": 5.4406624578180096e-05,
      "loss": 2.1345,
      "step": 142
    },
    {
      "epoch": 1.010600706713781,
      "grad_norm": 0.8892090916633606,
      "learning_rate": 5.382030910240936e-05,
      "loss": 1.6266,
      "step": 143
    },
    {
      "epoch": 1.017667844522968,
      "grad_norm": 0.7483237981796265,
      "learning_rate": 5.3233464730111426e-05,
      "loss": 1.7654,
      "step": 144
    },
    {
      "epoch": 1.0247349823321554,
      "grad_norm": 0.8178125619888306,
      "learning_rate": 5.2646172706008156e-05,
      "loss": 2.1553,
      "step": 145
    },
    {
      "epoch": 1.0318021201413428,
      "grad_norm": 0.8365805745124817,
      "learning_rate": 5.205851433679589e-05,
      "loss": 1.9679,
      "step": 146
    },
    {
      "epoch": 1.03886925795053,
      "grad_norm": 0.7663918137550354,
      "learning_rate": 5.1470570979888973e-05,
      "loss": 2.263,
      "step": 147
    },
    {
      "epoch": 1.0459363957597174,
      "grad_norm": 0.9471521973609924,
      "learning_rate": 5.088242403215644e-05,
      "loss": 1.975,
      "step": 148
    },
    {
      "epoch": 1.0530035335689045,
      "grad_norm": 0.9400817155838013,
      "learning_rate": 5.029415491865311e-05,
      "loss": 1.9287,
      "step": 149
    },
    {
      "epoch": 1.0600706713780919,
      "grad_norm": 0.8906810879707336,
      "learning_rate": 4.9705845081346894e-05,
      "loss": 2.0374,
      "step": 150
    },
    {
      "epoch": 1.0671378091872792,
      "grad_norm": 0.8151635527610779,
      "learning_rate": 4.911757596784357e-05,
      "loss": 1.7487,
      "step": 151
    },
    {
      "epoch": 1.0742049469964665,
      "grad_norm": 1.0287028551101685,
      "learning_rate": 4.852942902011103e-05,
      "loss": 2.4187,
      "step": 152
    },
    {
      "epoch": 1.0812720848056536,
      "grad_norm": 0.9997594952583313,
      "learning_rate": 4.7941485663204125e-05,
      "loss": 2.4478,
      "step": 153
    },
    {
      "epoch": 1.088339222614841,
      "grad_norm": 1.1276172399520874,
      "learning_rate": 4.735382729399184e-05,
      "loss": 2.4936,
      "step": 154
    },
    {
      "epoch": 1.0954063604240283,
      "grad_norm": 0.9459803104400635,
      "learning_rate": 4.676653526988858e-05,
      "loss": 1.8379,
      "step": 155
    },
    {
      "epoch": 1.1024734982332156,
      "grad_norm": 1.135962724685669,
      "learning_rate": 4.617969089759066e-05,
      "loss": 2.3317,
      "step": 156
    },
    {
      "epoch": 1.1095406360424027,
      "grad_norm": 1.1626211404800415,
      "learning_rate": 4.559337542181993e-05,
      "loss": 2.282,
      "step": 157
    },
    {
      "epoch": 1.11660777385159,
      "grad_norm": 1.2081414461135864,
      "learning_rate": 4.5007670014076045e-05,
      "loss": 2.5307,
      "step": 158
    },
    {
      "epoch": 1.1236749116607774,
      "grad_norm": 1.1183536052703857,
      "learning_rate": 4.442265576139878e-05,
      "loss": 2.3373,
      "step": 159
    },
    {
      "epoch": 1.1307420494699647,
      "grad_norm": 1.1179301738739014,
      "learning_rate": 4.383841365514208e-05,
      "loss": 2.3094,
      "step": 160
    },
    {
      "epoch": 1.137809187279152,
      "grad_norm": 1.193830132484436,
      "learning_rate": 4.325502457976126e-05,
      "loss": 2.1884,
      "step": 161
    },
    {
      "epoch": 1.1448763250883391,
      "grad_norm": 1.101815104484558,
      "learning_rate": 4.267256930161523e-05,
      "loss": 2.0212,
      "step": 162
    },
    {
      "epoch": 1.1519434628975265,
      "grad_norm": 1.2579357624053955,
      "learning_rate": 4.209112845778481e-05,
      "loss": 2.3388,
      "step": 163
    },
    {
      "epoch": 1.1590106007067138,
      "grad_norm": 1.4016278982162476,
      "learning_rate": 4.1510782544909075e-05,
      "loss": 2.3927,
      "step": 164
    },
    {
      "epoch": 1.1660777385159011,
      "grad_norm": 1.3191838264465332,
      "learning_rate": 4.09316119080412e-05,
      "loss": 2.5487,
      "step": 165
    },
    {
      "epoch": 1.1731448763250882,
      "grad_norm": 1.4483412504196167,
      "learning_rate": 4.035369672952516e-05,
      "loss": 2.4493,
      "step": 166
    },
    {
      "epoch": 1.1802120141342756,
      "grad_norm": 1.313664197921753,
      "learning_rate": 3.977711701789499e-05,
      "loss": 1.9944,
      "step": 167
    },
    {
      "epoch": 1.187279151943463,
      "grad_norm": 1.6458280086517334,
      "learning_rate": 3.920195259679822e-05,
      "loss": 2.6026,
      "step": 168
    },
    {
      "epoch": 1.1943462897526502,
      "grad_norm": 1.6923744678497314,
      "learning_rate": 3.8628283093944686e-05,
      "loss": 2.6408,
      "step": 169
    },
    {
      "epoch": 1.2014134275618376,
      "grad_norm": 1.8006504774093628,
      "learning_rate": 3.805618793008279e-05,
      "loss": 2.8821,
      "step": 170
    },
    {
      "epoch": 1.2084805653710247,
      "grad_norm": 1.8046975135803223,
      "learning_rate": 3.748574630800401e-05,
      "loss": 3.0376,
      "step": 171
    },
    {
      "epoch": 1.215547703180212,
      "grad_norm": 1.4958995580673218,
      "learning_rate": 3.691703720157798e-05,
      "loss": 2.3023,
      "step": 172
    },
    {
      "epoch": 1.2226148409893993,
      "grad_norm": 1.6216102838516235,
      "learning_rate": 3.635013934481895e-05,
      "loss": 2.4133,
      "step": 173
    },
    {
      "epoch": 1.2296819787985867,
      "grad_norm": 1.8731385469436646,
      "learning_rate": 3.578513122098566e-05,
      "loss": 2.3909,
      "step": 174
    },
    {
      "epoch": 1.2367491166077738,
      "grad_norm": 1.8722187280654907,
      "learning_rate": 3.52220910517158e-05,
      "loss": 2.611,
      "step": 175
    },
    {
      "epoch": 1.243816254416961,
      "grad_norm": 2.5984580516815186,
      "learning_rate": 3.466109678619681e-05,
      "loss": 2.524,
      "step": 176
    },
    {
      "epoch": 1.2508833922261484,
      "grad_norm": 2.304403781890869,
      "learning_rate": 3.4102226090374246e-05,
      "loss": 2.4895,
      "step": 177
    },
    {
      "epoch": 1.2579505300353357,
      "grad_norm": 0.9048846960067749,
      "learning_rate": 3.35455563361995e-05,
      "loss": 1.81,
      "step": 178
    },
    {
      "epoch": 1.265017667844523,
      "grad_norm": 0.8781378865242004,
      "learning_rate": 3.299116459091816e-05,
      "loss": 1.6648,
      "step": 179
    },
    {
      "epoch": 1.2720848056537102,
      "grad_norm": 0.8897349238395691,
      "learning_rate": 3.243912760640054e-05,
      "loss": 1.9141,
      "step": 180
    },
    {
      "epoch": 1.2791519434628975,
      "grad_norm": 0.9259485602378845,
      "learning_rate": 3.188952180851589e-05,
      "loss": 2.0695,
      "step": 181
    },
    {
      "epoch": 1.2862190812720848,
      "grad_norm": 0.9680337309837341,
      "learning_rate": 3.134242328655175e-05,
      "loss": 2.2456,
      "step": 182
    },
    {
      "epoch": 1.293286219081272,
      "grad_norm": 0.9842424392700195,
      "learning_rate": 3.079790778267994e-05,
      "loss": 2.0252,
      "step": 183
    },
    {
      "epoch": 1.3003533568904593,
      "grad_norm": 1.043618083000183,
      "learning_rate": 3.0256050681470444e-05,
      "loss": 2.1394,
      "step": 184
    },
    {
      "epoch": 1.3074204946996466,
      "grad_norm": 0.9687061905860901,
      "learning_rate": 2.971692699945502e-05,
      "loss": 2.3289,
      "step": 185
    },
    {
      "epoch": 1.314487632508834,
      "grad_norm": 0.8933001756668091,
      "learning_rate": 2.9180611374741623e-05,
      "loss": 2.0286,
      "step": 186
    },
    {
      "epoch": 1.3215547703180213,
      "grad_norm": 1.2500417232513428,
      "learning_rate": 2.8647178056681194e-05,
      "loss": 2.2341,
      "step": 187
    },
    {
      "epoch": 1.3286219081272086,
      "grad_norm": 1.2130661010742188,
      "learning_rate": 2.8116700895588472e-05,
      "loss": 2.3426,
      "step": 188
    },
    {
      "epoch": 1.3356890459363957,
      "grad_norm": 1.0185911655426025,
      "learning_rate": 2.7589253332517734e-05,
      "loss": 2.127,
      "step": 189
    },
    {
      "epoch": 1.342756183745583,
      "grad_norm": 1.0920605659484863,
      "learning_rate": 2.7064908389095468e-05,
      "loss": 2.2156,
      "step": 190
    },
    {
      "epoch": 1.3498233215547704,
      "grad_norm": 0.9638214707374573,
      "learning_rate": 2.6543738657411034e-05,
      "loss": 1.9115,
      "step": 191
    },
    {
      "epoch": 1.3568904593639575,
      "grad_norm": 1.0637297630310059,
      "learning_rate": 2.6025816289966704e-05,
      "loss": 2.1818,
      "step": 192
    },
    {
      "epoch": 1.3639575971731448,
      "grad_norm": 1.306837558746338,
      "learning_rate": 2.5511212989688586e-05,
      "loss": 2.0276,
      "step": 193
    },
    {
      "epoch": 1.3710247349823321,
      "grad_norm": 1.2165682315826416,
      "learning_rate": 2.500000000000001e-05,
      "loss": 2.1713,
      "step": 194
    },
    {
      "epoch": 1.3780918727915195,
      "grad_norm": 1.1613742113113403,
      "learning_rate": 2.4492248094958147e-05,
      "loss": 2.2518,
      "step": 195
    },
    {
      "epoch": 1.3851590106007068,
      "grad_norm": 1.2424681186676025,
      "learning_rate": 2.3988027569455895e-05,
      "loss": 2.3013,
      "step": 196
    },
    {
      "epoch": 1.3922261484098941,
      "grad_norm": 1.0724608898162842,
      "learning_rate": 2.348740822949006e-05,
      "loss": 2.1352,
      "step": 197
    },
    {
      "epoch": 1.3992932862190812,
      "grad_norm": 1.1237046718597412,
      "learning_rate": 2.2990459382497088e-05,
      "loss": 1.8704,
      "step": 198
    },
    {
      "epoch": 1.4063604240282686,
      "grad_norm": 1.2052812576293945,
      "learning_rate": 2.2497249827757933e-05,
      "loss": 2.2533,
      "step": 199
    },
    {
      "epoch": 1.4134275618374559,
      "grad_norm": 1.4499742984771729,
      "learning_rate": 2.200784784687334e-05,
      "loss": 2.4447,
      "step": 200
    },
    {
      "epoch": 1.420494699646643,
      "grad_norm": 1.3963183164596558,
      "learning_rate": 2.1522321194310574e-05,
      "loss": 2.4529,
      "step": 201
    },
    {
      "epoch": 1.4275618374558303,
      "grad_norm": 1.4335461854934692,
      "learning_rate": 2.1040737088023323e-05,
      "loss": 2.4491,
      "step": 202
    },
    {
      "epoch": 1.4346289752650176,
      "grad_norm": 1.3473395109176636,
      "learning_rate": 2.056316220014588e-05,
      "loss": 2.4169,
      "step": 203
    },
    {
      "epoch": 1.441696113074205,
      "grad_norm": 1.492969274520874,
      "learning_rate": 2.0089662647762715e-05,
      "loss": 2.6738,
      "step": 204
    },
    {
      "epoch": 1.4487632508833923,
      "grad_norm": 1.621220350265503,
      "learning_rate": 1.962030398375506e-05,
      "loss": 2.4748,
      "step": 205
    },
    {
      "epoch": 1.4558303886925796,
      "grad_norm": 1.652695894241333,
      "learning_rate": 1.9155151187725552e-05,
      "loss": 2.738,
      "step": 206
    },
    {
      "epoch": 1.4628975265017667,
      "grad_norm": 1.7737561464309692,
      "learning_rate": 1.8694268657002194e-05,
      "loss": 2.8505,
      "step": 207
    },
    {
      "epoch": 1.469964664310954,
      "grad_norm": 1.6658512353897095,
      "learning_rate": 1.8237720197723075e-05,
      "loss": 2.491,
      "step": 208
    },
    {
      "epoch": 1.4770318021201414,
      "grad_norm": 2.0604076385498047,
      "learning_rate": 1.7785569016002685e-05,
      "loss": 2.5712,
      "step": 209
    },
    {
      "epoch": 1.4840989399293285,
      "grad_norm": 2.5598301887512207,
      "learning_rate": 1.7337877709181526e-05,
      "loss": 3.2844,
      "step": 210
    },
    {
      "epoch": 1.4911660777385158,
      "grad_norm": 2.6366183757781982,
      "learning_rate": 1.689470825715998e-05,
      "loss": 3.2531,
      "step": 211
    },
    {
      "epoch": 1.4982332155477032,
      "grad_norm": 3.2231693267822266,
      "learning_rate": 1.6456122013817476e-05,
      "loss": 2.4167,
      "step": 212
    },
    {
      "epoch": 1.5053003533568905,
      "grad_norm": 1.050787329673767,
      "learning_rate": 1.6022179698518523e-05,
      "loss": 1.6225,
      "step": 213
    },
    {
      "epoch": 1.5123674911660778,
      "grad_norm": 0.8234966993331909,
      "learning_rate": 1.559294138770656e-05,
      "loss": 1.7362,
      "step": 214
    },
    {
      "epoch": 1.5194346289752652,
      "grad_norm": 1.1330937147140503,
      "learning_rate": 1.5168466506586654e-05,
      "loss": 2.1794,
      "step": 215
    },
    {
      "epoch": 1.5265017667844523,
      "grad_norm": 1.0325876474380493,
      "learning_rate": 1.4748813820898554e-05,
      "loss": 1.9843,
      "step": 216
    },
    {
      "epoch": 1.5335689045936396,
      "grad_norm": 0.9840700626373291,
      "learning_rate": 1.4334041428781003e-05,
      "loss": 2.0485,
      "step": 217
    },
    {
      "epoch": 1.5406360424028267,
      "grad_norm": 0.9490745067596436,
      "learning_rate": 1.3924206752728281e-05,
      "loss": 2.1239,
      "step": 218
    },
    {
      "epoch": 1.547703180212014,
      "grad_norm": 0.9429057836532593,
      "learning_rate": 1.3519366531640582e-05,
      "loss": 2.0955,
      "step": 219
    },
    {
      "epoch": 1.5547703180212014,
      "grad_norm": 1.0594981908798218,
      "learning_rate": 1.3119576812968892e-05,
      "loss": 2.1839,
      "step": 220
    },
    {
      "epoch": 1.5618374558303887,
      "grad_norm": 1.1968638896942139,
      "learning_rate": 1.272489294495548e-05,
      "loss": 2.0367,
      "step": 221
    },
    {
      "epoch": 1.568904593639576,
      "grad_norm": 1.03670072555542,
      "learning_rate": 1.233536956897136e-05,
      "loss": 2.0874,
      "step": 222
    },
    {
      "epoch": 1.5759717314487633,
      "grad_norm": 1.226658821105957,
      "learning_rate": 1.1951060611951615e-05,
      "loss": 2.4139,
      "step": 223
    },
    {
      "epoch": 1.5830388692579507,
      "grad_norm": 0.9740089774131775,
      "learning_rate": 1.1572019278929458e-05,
      "loss": 1.8154,
      "step": 224
    },
    {
      "epoch": 1.5901060070671378,
      "grad_norm": 0.9918175339698792,
      "learning_rate": 1.1198298045670402e-05,
      "loss": 1.5994,
      "step": 225
    },
    {
      "epoch": 1.5971731448763251,
      "grad_norm": 1.02961003780365,
      "learning_rate": 1.0829948651407374e-05,
      "loss": 1.9736,
      "step": 226
    },
    {
      "epoch": 1.6042402826855122,
      "grad_norm": 1.1128182411193848,
      "learning_rate": 1.0467022091677691e-05,
      "loss": 2.0124,
      "step": 227
    },
    {
      "epoch": 1.6113074204946995,
      "grad_norm": 1.2958669662475586,
      "learning_rate": 1.0109568611263093e-05,
      "loss": 1.9595,
      "step": 228
    },
    {
      "epoch": 1.6183745583038869,
      "grad_norm": 1.1893036365509033,
      "learning_rate": 9.75763769723373e-06,
      "loss": 2.0183,
      "step": 229
    },
    {
      "epoch": 1.6254416961130742,
      "grad_norm": 1.2391008138656616,
      "learning_rate": 9.41127807209688e-06,
      "loss": 2.3147,
      "step": 230
    },
    {
      "epoch": 1.6325088339222615,
      "grad_norm": 1.267835021018982,
      "learning_rate": 9.070537687051817e-06,
      "loss": 2.4213,
      "step": 231
    },
    {
      "epoch": 1.6395759717314489,
      "grad_norm": 1.320273995399475,
      "learning_rate": 8.735463715351139e-06,
      "loss": 2.6123,
      "step": 232
    },
    {
      "epoch": 1.6466431095406362,
      "grad_norm": 1.259988784790039,
      "learning_rate": 8.406102545769989e-06,
      "loss": 2.1121,
      "step": 233
    },
    {
      "epoch": 1.6537102473498233,
      "grad_norm": 1.2556949853897095,
      "learning_rate": 8.082499776183883e-06,
      "loss": 2.1004,
      "step": 234
    },
    {
      "epoch": 1.6607773851590106,
      "grad_norm": 1.5419262647628784,
      "learning_rate": 7.764700207255903e-06,
      "loss": 2.5967,
      "step": 235
    },
    {
      "epoch": 1.6678445229681977,
      "grad_norm": 1.446892499923706,
      "learning_rate": 7.452747836234392e-06,
      "loss": 2.4068,
      "step": 236
    },
    {
      "epoch": 1.674911660777385,
      "grad_norm": 1.6502734422683716,
      "learning_rate": 7.146685850861851e-06,
      "loss": 2.4744,
      "step": 237
    },
    {
      "epoch": 1.6819787985865724,
      "grad_norm": 1.5195056200027466,
      "learning_rate": 6.8465566233957945e-06,
      "loss": 2.6081,
      "step": 238
    },
    {
      "epoch": 1.6890459363957597,
      "grad_norm": 1.477294683456421,
      "learning_rate": 6.552401704742678e-06,
      "loss": 2.1462,
      "step": 239
    },
    {
      "epoch": 1.696113074204947,
      "grad_norm": 1.6022415161132812,
      "learning_rate": 6.264261818705419e-06,
      "loss": 2.7564,
      "step": 240
    },
    {
      "epoch": 1.7031802120141344,
      "grad_norm": 1.6184418201446533,
      "learning_rate": 5.982176856345445e-06,
      "loss": 2.5322,
      "step": 241
    },
    {
      "epoch": 1.7102473498233217,
      "grad_norm": 1.8783550262451172,
      "learning_rate": 5.706185870460018e-06,
      "loss": 2.8304,
      "step": 242
    },
    {
      "epoch": 1.7173144876325088,
      "grad_norm": 1.6531902551651,
      "learning_rate": 5.436327070175728e-06,
      "loss": 2.5404,
      "step": 243
    },
    {
      "epoch": 1.7243816254416962,
      "grad_norm": 1.9372646808624268,
      "learning_rate": 5.1726378156585816e-06,
      "loss": 2.5027,
      "step": 244
    },
    {
      "epoch": 1.7314487632508833,
      "grad_norm": 2.967853307723999,
      "learning_rate": 4.9151546129417804e-06,
      "loss": 2.1734,
      "step": 245
    },
    {
      "epoch": 1.7385159010600706,
      "grad_norm": 2.758082389831543,
      "learning_rate": 4.663913108871726e-06,
      "loss": 3.4049,
      "step": 246
    },
    {
      "epoch": 1.745583038869258,
      "grad_norm": 3.1087844371795654,
      "learning_rate": 4.418948086172914e-06,
      "loss": 2.7301,
      "step": 247
    },
    {
      "epoch": 1.7526501766784452,
      "grad_norm": 0.9271235466003418,
      "learning_rate": 4.180293458632489e-06,
      "loss": 1.5934,
      "step": 248
    },
    {
      "epoch": 1.7597173144876326,
      "grad_norm": 1.0243284702301025,
      "learning_rate": 3.947982266405159e-06,
      "loss": 1.8074,
      "step": 249
    },
    {
      "epoch": 1.76678445229682,
      "grad_norm": 0.9845365881919861,
      "learning_rate": 3.72204667143895e-06,
      "loss": 2.1799,
      "step": 250
    },
    {
      "epoch": 1.773851590106007,
      "grad_norm": 0.941881537437439,
      "learning_rate": 3.5025179530225994e-06,
      "loss": 2.1842,
      "step": 251
    },
    {
      "epoch": 1.7809187279151943,
      "grad_norm": 0.9159271121025085,
      "learning_rate": 3.289426503455201e-06,
      "loss": 1.9122,
      "step": 252
    },
    {
      "epoch": 1.7879858657243817,
      "grad_norm": 0.9244300723075867,
      "learning_rate": 3.082801823838527e-06,
      "loss": 1.5459,
      "step": 253
    },
    {
      "epoch": 1.7950530035335688,
      "grad_norm": 0.9460816383361816,
      "learning_rate": 2.882672519992824e-06,
      "loss": 1.7899,
      "step": 254
    },
    {
      "epoch": 1.802120141342756,
      "grad_norm": 1.09010648727417,
      "learning_rate": 2.6890662984965232e-06,
      "loss": 2.0719,
      "step": 255
    },
    {
      "epoch": 1.8091872791519434,
      "grad_norm": 1.0472935438156128,
      "learning_rate": 2.50200996285046e-06,
      "loss": 1.7891,
      "step": 256
    },
    {
      "epoch": 1.8162544169611308,
      "grad_norm": 1.0949760675430298,
      "learning_rate": 2.3215294097670925e-06,
      "loss": 2.0959,
      "step": 257
    },
    {
      "epoch": 1.823321554770318,
      "grad_norm": 0.9531151652336121,
      "learning_rate": 2.1476496255852683e-06,
      "loss": 1.8291,
      "step": 258
    },
    {
      "epoch": 1.8303886925795054,
      "grad_norm": 1.1798701286315918,
      "learning_rate": 1.9803946828110375e-06,
      "loss": 2.2515,
      "step": 259
    },
    {
      "epoch": 1.8374558303886925,
      "grad_norm": 1.3413084745407104,
      "learning_rate": 1.8197877367849947e-06,
      "loss": 2.1723,
      "step": 260
    },
    {
      "epoch": 1.8445229681978799,
      "grad_norm": 1.042053461074829,
      "learning_rate": 1.6658510224765333e-06,
      "loss": 1.9748,
      "step": 261
    },
    {
      "epoch": 1.851590106007067,
      "grad_norm": 1.1710350513458252,
      "learning_rate": 1.5186058514055912e-06,
      "loss": 2.0839,
      "step": 262
    },
    {
      "epoch": 1.8586572438162543,
      "grad_norm": 1.106331706047058,
      "learning_rate": 1.3780726086922103e-06,
      "loss": 2.2887,
      "step": 263
    },
    {
      "epoch": 1.8657243816254416,
      "grad_norm": 1.1169514656066895,
      "learning_rate": 1.2442707502343332e-06,
      "loss": 2.0647,
      "step": 264
    },
    {
      "epoch": 1.872791519434629,
      "grad_norm": 1.4165184497833252,
      "learning_rate": 1.1172188000142802e-06,
      "loss": 2.416,
      "step": 265
    },
    {
      "epoch": 1.8798586572438163,
      "grad_norm": 1.3979625701904297,
      "learning_rate": 9.969343475342285e-07,
      "loss": 2.3506,
      "step": 266
    },
    {
      "epoch": 1.8869257950530036,
      "grad_norm": 1.1879550218582153,
      "learning_rate": 8.834340453810375e-07,
      "loss": 1.7977,
      "step": 267
    },
    {
      "epoch": 1.893992932862191,
      "grad_norm": 1.2774851322174072,
      "learning_rate": 7.76733606920832e-07,
      "loss": 2.2051,
      "step": 268
    },
    {
      "epoch": 1.901060070671378,
      "grad_norm": 1.335674524307251,
      "learning_rate": 6.768478041236037e-07,
      "loss": 2.8301,
      "step": 269
    },
    {
      "epoch": 1.9081272084805654,
      "grad_norm": 1.1923797130584717,
      "learning_rate": 5.837904655180748e-07,
      "loss": 2.2285,
      "step": 270
    },
    {
      "epoch": 1.9151943462897525,
      "grad_norm": 1.3273406028747559,
      "learning_rate": 4.975744742772848e-07,
      "loss": 2.282,
      "step": 271
    },
    {
      "epoch": 1.9222614840989398,
      "grad_norm": 1.3906129598617554,
      "learning_rate": 4.182117664349783e-07,
      "loss": 2.4653,
      "step": 272
    },
    {
      "epoch": 1.9293286219081272,
      "grad_norm": 1.3172883987426758,
      "learning_rate": 3.4571332923314936e-07,
      "loss": 2.3578,
      "step": 273
    },
    {
      "epoch": 1.9363957597173145,
      "grad_norm": 1.4335203170776367,
      "learning_rate": 2.800891996009025e-07,
      "loss": 2.1652,
      "step": 274
    },
    {
      "epoch": 1.9434628975265018,
      "grad_norm": 1.5946608781814575,
      "learning_rate": 2.2134846276494202e-07,
      "loss": 2.5576,
      "step": 275
    },
    {
      "epoch": 1.9505300353356891,
      "grad_norm": 1.7991340160369873,
      "learning_rate": 1.69499250991767e-07,
      "loss": 2.4865,
      "step": 276
    },
    {
      "epoch": 1.9575971731448765,
      "grad_norm": 1.8213907480239868,
      "learning_rate": 1.245487424618108e-07,
      "loss": 2.5855,
      "step": 277
    },
    {
      "epoch": 1.9646643109540636,
      "grad_norm": 1.8912492990493774,
      "learning_rate": 8.650316027566386e-08,
      "loss": 2.7235,
      "step": 278
    },
    {
      "epoch": 1.971731448763251,
      "grad_norm": 2.2245891094207764,
      "learning_rate": 5.536777159254603e-08,
      "loss": 2.6578,
      "step": 279
    },
    {
      "epoch": 1.978798586572438,
      "grad_norm": 2.1335883140563965,
      "learning_rate": 3.1146886901090025e-08,
      "loss": 2.1648,
      "step": 280
    },
    {
      "epoch": 1.9858657243816253,
      "grad_norm": 2.6661763191223145,
      "learning_rate": 1.3843859422574268e-08,
      "loss": 3.1698,
      "step": 281
    },
    {
      "epoch": 1.9929328621908127,
      "grad_norm": 2.128255844116211,
      "learning_rate": 3.4610846467109103e-09,
      "loss": 2.2754,
      "step": 282
    }
  ],
  "logging_steps": 1,
  "max_steps": 282,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 3656473928048640.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
