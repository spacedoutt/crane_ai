{
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9964664310954063,
  "eval_steps": 500,
  "global_step": 141,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.007067137809187279,
      "grad_norm": 2.022409439086914,
      "learning_rate": 6.666666666666667e-06,
      "loss": 2.4494,
      "step": 1
    },
    {
      "epoch": 0.014134275618374558,
      "grad_norm": 0.9793543219566345,
      "learning_rate": 1.3333333333333333e-05,
      "loss": 2.5234,
      "step": 2
    },
    {
      "epoch": 0.02120141342756184,
      "grad_norm": 1.6935573816299438,
      "learning_rate": 2e-05,
      "loss": 3.0289,
      "step": 3
    },
    {
      "epoch": 0.028268551236749116,
      "grad_norm": 1.7486528158187866,
      "learning_rate": 2.6666666666666667e-05,
      "loss": 3.0376,
      "step": 4
    },
    {
      "epoch": 0.0353356890459364,
      "grad_norm": 1.5069012641906738,
      "learning_rate": 3.3333333333333335e-05,
      "loss": 2.8815,
      "step": 5
    },
    {
      "epoch": 0.04240282685512368,
      "grad_norm": 2.1140050888061523,
      "learning_rate": 4e-05,
      "loss": 3.136,
      "step": 6
    },
    {
      "epoch": 0.04946996466431095,
      "grad_norm": 1.7252628803253174,
      "learning_rate": 4.666666666666667e-05,
      "loss": 3.1061,
      "step": 7
    },
    {
      "epoch": 0.05653710247349823,
      "grad_norm": 2.205775260925293,
      "learning_rate": 5.333333333333333e-05,
      "loss": 3.0103,
      "step": 8
    },
    {
      "epoch": 0.0636042402826855,
      "grad_norm": 2.2606282234191895,
      "learning_rate": 6e-05,
      "loss": 3.185,
      "step": 9
    },
    {
      "epoch": 0.0706713780918728,
      "grad_norm": 1.8141779899597168,
      "learning_rate": 6.666666666666667e-05,
      "loss": 2.7712,
      "step": 10
    },
    {
      "epoch": 0.07773851590106007,
      "grad_norm": 2.1690306663513184,
      "learning_rate": 7.333333333333333e-05,
      "loss": 3.3446,
      "step": 11
    },
    {
      "epoch": 0.08480565371024736,
      "grad_norm": 1.9139608144760132,
      "learning_rate": 8e-05,
      "loss": 2.8999,
      "step": 12
    },
    {
      "epoch": 0.09187279151943463,
      "grad_norm": 1.8831859827041626,
      "learning_rate": 8.666666666666667e-05,
      "loss": 2.8619,
      "step": 13
    },
    {
      "epoch": 0.0989399293286219,
      "grad_norm": 1.8665746450424194,
      "learning_rate": 9.333333333333334e-05,
      "loss": 2.8851,
      "step": 14
    },
    {
      "epoch": 0.10600706713780919,
      "grad_norm": 2.231839656829834,
      "learning_rate": 0.0001,
      "loss": 3.11,
      "step": 15
    },
    {
      "epoch": 0.11307420494699646,
      "grad_norm": 2.2183234691619873,
      "learning_rate": 9.99965389153533e-05,
      "loss": 3.059,
      "step": 16
    },
    {
      "epoch": 0.12014134275618374,
      "grad_norm": 1.883208155632019,
      "learning_rate": 9.998615614057742e-05,
      "loss": 2.544,
      "step": 17
    },
    {
      "epoch": 0.127208480565371,
      "grad_norm": 1.95951247215271,
      "learning_rate": 9.996885311309891e-05,
      "loss": 2.7836,
      "step": 18
    },
    {
      "epoch": 0.13427561837455831,
      "grad_norm": 1.8068021535873413,
      "learning_rate": 9.994463222840746e-05,
      "loss": 2.6707,
      "step": 19
    },
    {
      "epoch": 0.1413427561837456,
      "grad_norm": 1.9451733827590942,
      "learning_rate": 9.991349683972434e-05,
      "loss": 2.9288,
      "step": 20
    },
    {
      "epoch": 0.14840989399293286,
      "grad_norm": 1.9017914533615112,
      "learning_rate": 9.987545125753819e-05,
      "loss": 2.3736,
      "step": 21
    },
    {
      "epoch": 0.15547703180212014,
      "grad_norm": 1.8215432167053223,
      "learning_rate": 9.983050074900824e-05,
      "loss": 2.7856,
      "step": 22
    },
    {
      "epoch": 0.1625441696113074,
      "grad_norm": 1.870759129524231,
      "learning_rate": 9.977865153723507e-05,
      "loss": 2.7586,
      "step": 23
    },
    {
      "epoch": 0.1696113074204947,
      "grad_norm": 2.056816577911377,
      "learning_rate": 9.97199108003991e-05,
      "loss": 2.8988,
      "step": 24
    },
    {
      "epoch": 0.17667844522968199,
      "grad_norm": 2.137852668762207,
      "learning_rate": 9.965428667076686e-05,
      "loss": 3.3246,
      "step": 25
    },
    {
      "epoch": 0.18374558303886926,
      "grad_norm": 1.8751949071884155,
      "learning_rate": 9.958178823356503e-05,
      "loss": 2.5371,
      "step": 26
    },
    {
      "epoch": 0.19081272084805653,
      "grad_norm": 2.405423879623413,
      "learning_rate": 9.950242552572271e-05,
      "loss": 2.7347,
      "step": 27
    },
    {
      "epoch": 0.1978798586572438,
      "grad_norm": 3.065908670425415,
      "learning_rate": 9.941620953448194e-05,
      "loss": 2.9675,
      "step": 28
    },
    {
      "epoch": 0.2049469964664311,
      "grad_norm": 2.038029909133911,
      "learning_rate": 9.93231521958764e-05,
      "loss": 2.6089,
      "step": 29
    },
    {
      "epoch": 0.21201413427561838,
      "grad_norm": 1.9041630029678345,
      "learning_rate": 9.922326639307917e-05,
      "loss": 3.1188,
      "step": 30
    },
    {
      "epoch": 0.21908127208480566,
      "grad_norm": 2.1505870819091797,
      "learning_rate": 9.911656595461898e-05,
      "loss": 2.7294,
      "step": 31
    },
    {
      "epoch": 0.22614840989399293,
      "grad_norm": 2.1394741535186768,
      "learning_rate": 9.900306565246578e-05,
      "loss": 2.7965,
      "step": 32
    },
    {
      "epoch": 0.2332155477031802,
      "grad_norm": 2.3517096042633057,
      "learning_rate": 9.888278119998573e-05,
      "loss": 2.7225,
      "step": 33
    },
    {
      "epoch": 0.24028268551236748,
      "grad_norm": 2.5036532878875732,
      "learning_rate": 9.875572924976568e-05,
      "loss": 3.004,
      "step": 34
    },
    {
      "epoch": 0.24734982332155478,
      "grad_norm": 4.801952362060547,
      "learning_rate": 9.86219273913078e-05,
      "loss": 3.9754,
      "step": 35
    },
    {
      "epoch": 0.254416961130742,
      "grad_norm": 1.7934314012527466,
      "learning_rate": 9.848139414859441e-05,
      "loss": 2.1512,
      "step": 36
    },
    {
      "epoch": 0.26148409893992935,
      "grad_norm": 1.6315264701843262,
      "learning_rate": 9.833414897752347e-05,
      "loss": 2.171,
      "step": 37
    },
    {
      "epoch": 0.26855123674911663,
      "grad_norm": 1.4425857067108154,
      "learning_rate": 9.8180212263215e-05,
      "loss": 2.1051,
      "step": 38
    },
    {
      "epoch": 0.2756183745583039,
      "grad_norm": 1.155803918838501,
      "learning_rate": 9.801960531718896e-05,
      "loss": 2.4763,
      "step": 39
    },
    {
      "epoch": 0.2826855123674912,
      "grad_norm": 1.0320155620574951,
      "learning_rate": 9.785235037441474e-05,
      "loss": 2.199,
      "step": 40
    },
    {
      "epoch": 0.28975265017667845,
      "grad_norm": 0.9723770618438721,
      "learning_rate": 9.767847059023291e-05,
      "loss": 2.2018,
      "step": 41
    },
    {
      "epoch": 0.2968197879858657,
      "grad_norm": 1.0566339492797852,
      "learning_rate": 9.749799003714954e-05,
      "loss": 2.2219,
      "step": 42
    },
    {
      "epoch": 0.303886925795053,
      "grad_norm": 1.1914960145950317,
      "learning_rate": 9.731093370150349e-05,
      "loss": 2.0384,
      "step": 43
    },
    {
      "epoch": 0.31095406360424027,
      "grad_norm": 1.0611823797225952,
      "learning_rate": 9.71173274800072e-05,
      "loss": 2.4735,
      "step": 44
    },
    {
      "epoch": 0.31802120141342755,
      "grad_norm": 0.922407865524292,
      "learning_rate": 9.691719817616147e-05,
      "loss": 2.3903,
      "step": 45
    },
    {
      "epoch": 0.3250883392226148,
      "grad_norm": 0.9721489548683167,
      "learning_rate": 9.67105734965448e-05,
      "loss": 2.3799,
      "step": 46
    },
    {
      "epoch": 0.3321554770318021,
      "grad_norm": 0.9984425902366638,
      "learning_rate": 9.64974820469774e-05,
      "loss": 2.2864,
      "step": 47
    },
    {
      "epoch": 0.3392226148409894,
      "grad_norm": 1.0947078466415405,
      "learning_rate": 9.627795332856107e-05,
      "loss": 2.6168,
      "step": 48
    },
    {
      "epoch": 0.3462897526501767,
      "grad_norm": 1.1075009107589722,
      "learning_rate": 9.605201773359485e-05,
      "loss": 2.3285,
      "step": 49
    },
    {
      "epoch": 0.35335689045936397,
      "grad_norm": 1.056508183479309,
      "learning_rate": 9.581970654136751e-05,
      "loss": 1.9736,
      "step": 50
    },
    {
      "epoch": 0.36042402826855124,
      "grad_norm": 1.2873791456222534,
      "learning_rate": 9.55810519138271e-05,
      "loss": 2.6025,
      "step": 51
    },
    {
      "epoch": 0.3674911660777385,
      "grad_norm": 1.0405802726745605,
      "learning_rate": 9.533608689112827e-05,
      "loss": 2.0896,
      "step": 52
    },
    {
      "epoch": 0.3745583038869258,
      "grad_norm": 1.2283798456192017,
      "learning_rate": 9.508484538705824e-05,
      "loss": 2.4158,
      "step": 53
    },
    {
      "epoch": 0.38162544169611307,
      "grad_norm": 1.3034777641296387,
      "learning_rate": 9.482736218434143e-05,
      "loss": 2.6009,
      "step": 54
    },
    {
      "epoch": 0.38869257950530034,
      "grad_norm": 1.1551556587219238,
      "learning_rate": 9.456367292982429e-05,
      "loss": 2.3453,
      "step": 55
    },
    {
      "epoch": 0.3957597173144876,
      "grad_norm": 1.1859220266342163,
      "learning_rate": 9.429381412953999e-05,
      "loss": 2.2731,
      "step": 56
    },
    {
      "epoch": 0.4028268551236749,
      "grad_norm": 1.333423137664795,
      "learning_rate": 9.401782314365457e-05,
      "loss": 2.3263,
      "step": 57
    },
    {
      "epoch": 0.4098939929328622,
      "grad_norm": 1.5633811950683594,
      "learning_rate": 9.373573818129458e-05,
      "loss": 2.5062,
      "step": 58
    },
    {
      "epoch": 0.4169611307420495,
      "grad_norm": 1.2807029485702515,
      "learning_rate": 9.344759829525733e-05,
      "loss": 2.5807,
      "step": 59
    },
    {
      "epoch": 0.42402826855123676,
      "grad_norm": 1.6720954179763794,
      "learning_rate": 9.315344337660421e-05,
      "loss": 2.8877,
      "step": 60
    },
    {
      "epoch": 0.43109540636042404,
      "grad_norm": 1.7105133533477783,
      "learning_rate": 9.285331414913815e-05,
      "loss": 2.9115,
      "step": 61
    },
    {
      "epoch": 0.4381625441696113,
      "grad_norm": 1.6162095069885254,
      "learning_rate": 9.254725216376561e-05,
      "loss": 2.6416,
      "step": 62
    },
    {
      "epoch": 0.4452296819787986,
      "grad_norm": 1.718125820159912,
      "learning_rate": 9.22352997927441e-05,
      "loss": 3.2215,
      "step": 63
    },
    {
      "epoch": 0.45229681978798586,
      "grad_norm": 1.78681480884552,
      "learning_rate": 9.191750022381614e-05,
      "loss": 3.0858,
      "step": 64
    },
    {
      "epoch": 0.45936395759717313,
      "grad_norm": 1.4100565910339355,
      "learning_rate": 9.159389745423002e-05,
      "loss": 2.9017,
      "step": 65
    },
    {
      "epoch": 0.4664310954063604,
      "grad_norm": 1.4898382425308228,
      "learning_rate": 9.126453628464888e-05,
      "loss": 2.6049,
      "step": 66
    },
    {
      "epoch": 0.4734982332155477,
      "grad_norm": 1.5636359453201294,
      "learning_rate": 9.092946231294819e-05,
      "loss": 2.7736,
      "step": 67
    },
    {
      "epoch": 0.48056537102473496,
      "grad_norm": 1.6875892877578735,
      "learning_rate": 9.058872192790313e-05,
      "loss": 2.6217,
      "step": 68
    },
    {
      "epoch": 0.4876325088339223,
      "grad_norm": 2.276315450668335,
      "learning_rate": 9.024236230276629e-05,
      "loss": 3.3577,
      "step": 69
    },
    {
      "epoch": 0.49469964664310956,
      "grad_norm": 3.0389819145202637,
      "learning_rate": 8.98904313887369e-05,
      "loss": 2.9188,
      "step": 70
    },
    {
      "epoch": 0.5017667844522968,
      "grad_norm": 0.970057487487793,
      "learning_rate": 8.953297790832231e-05,
      "loss": 1.8327,
      "step": 71
    },
    {
      "epoch": 0.508833922261484,
      "grad_norm": 1.0968877077102661,
      "learning_rate": 8.917005134859263e-05,
      "loss": 1.8808,
      "step": 72
    },
    {
      "epoch": 0.5159010600706714,
      "grad_norm": 1.0374150276184082,
      "learning_rate": 8.88017019543296e-05,
      "loss": 2.1717,
      "step": 73
    },
    {
      "epoch": 0.5229681978798587,
      "grad_norm": 1.0458382368087769,
      "learning_rate": 8.842798072107054e-05,
      "loss": 2.0629,
      "step": 74
    },
    {
      "epoch": 0.5300353356890459,
      "grad_norm": 1.0578193664550781,
      "learning_rate": 8.80489393880484e-05,
      "loss": 2.4489,
      "step": 75
    },
    {
      "epoch": 0.5371024734982333,
      "grad_norm": 1.0196895599365234,
      "learning_rate": 8.766463043102864e-05,
      "loss": 1.9512,
      "step": 76
    },
    {
      "epoch": 0.5441696113074205,
      "grad_norm": 0.9518066644668579,
      "learning_rate": 8.727510705504454e-05,
      "loss": 1.8802,
      "step": 77
    },
    {
      "epoch": 0.5512367491166078,
      "grad_norm": 0.8494212627410889,
      "learning_rate": 8.688042318703111e-05,
      "loss": 2.2707,
      "step": 78
    },
    {
      "epoch": 0.558303886925795,
      "grad_norm": 0.9143539071083069,
      "learning_rate": 8.648063346835942e-05,
      "loss": 2.0863,
      "step": 79
    },
    {
      "epoch": 0.5653710247349824,
      "grad_norm": 0.9092293381690979,
      "learning_rate": 8.607579324727175e-05,
      "loss": 2.0571,
      "step": 80
    },
    {
      "epoch": 0.5724381625441696,
      "grad_norm": 0.9530013203620911,
      "learning_rate": 8.566595857121902e-05,
      "loss": 2.3529,
      "step": 81
    },
    {
      "epoch": 0.5795053003533569,
      "grad_norm": 0.9184759855270386,
      "learning_rate": 8.525118617910143e-05,
      "loss": 1.9959,
      "step": 82
    },
    {
      "epoch": 0.5865724381625441,
      "grad_norm": 0.9186165928840637,
      "learning_rate": 8.483153349341335e-05,
      "loss": 2.0539,
      "step": 83
    },
    {
      "epoch": 0.5936395759717314,
      "grad_norm": 1.0846062898635864,
      "learning_rate": 8.440705861229344e-05,
      "loss": 1.8918,
      "step": 84
    },
    {
      "epoch": 0.6007067137809188,
      "grad_norm": 1.2125283479690552,
      "learning_rate": 8.397782030148147e-05,
      "loss": 2.1233,
      "step": 85
    },
    {
      "epoch": 0.607773851590106,
      "grad_norm": 1.1552107334136963,
      "learning_rate": 8.354387798618253e-05,
      "loss": 2.6566,
      "step": 86
    },
    {
      "epoch": 0.6148409893992933,
      "grad_norm": 1.175605058670044,
      "learning_rate": 8.310529174284004e-05,
      "loss": 2.4801,
      "step": 87
    },
    {
      "epoch": 0.6219081272084805,
      "grad_norm": 1.2986605167388916,
      "learning_rate": 8.266212229081847e-05,
      "loss": 2.5867,
      "step": 88
    },
    {
      "epoch": 0.6289752650176679,
      "grad_norm": 1.364498496055603,
      "learning_rate": 8.221443098399732e-05,
      "loss": 2.7839,
      "step": 89
    },
    {
      "epoch": 0.6360424028268551,
      "grad_norm": 1.1776494979858398,
      "learning_rate": 8.176227980227694e-05,
      "loss": 2.0666,
      "step": 90
    },
    {
      "epoch": 0.6431095406360424,
      "grad_norm": 1.3584233522415161,
      "learning_rate": 8.130573134299782e-05,
      "loss": 2.5463,
      "step": 91
    },
    {
      "epoch": 0.6501766784452296,
      "grad_norm": 1.1593364477157593,
      "learning_rate": 8.084484881227448e-05,
      "loss": 2.3973,
      "step": 92
    },
    {
      "epoch": 0.657243816254417,
      "grad_norm": 1.3711793422698975,
      "learning_rate": 8.037969601624495e-05,
      "loss": 2.5453,
      "step": 93
    },
    {
      "epoch": 0.6643109540636042,
      "grad_norm": 1.0680524110794067,
      "learning_rate": 7.991033735223729e-05,
      "loss": 2.3226,
      "step": 94
    },
    {
      "epoch": 0.6713780918727915,
      "grad_norm": 1.5485703945159912,
      "learning_rate": 7.943683779985413e-05,
      "loss": 2.4686,
      "step": 95
    },
    {
      "epoch": 0.6784452296819788,
      "grad_norm": 1.2072105407714844,
      "learning_rate": 7.895926291197667e-05,
      "loss": 2.239,
      "step": 96
    },
    {
      "epoch": 0.6855123674911661,
      "grad_norm": 1.3206794261932373,
      "learning_rate": 7.847767880568945e-05,
      "loss": 2.6581,
      "step": 97
    },
    {
      "epoch": 0.6925795053003534,
      "grad_norm": 1.349073052406311,
      "learning_rate": 7.799215215312667e-05,
      "loss": 2.5845,
      "step": 98
    },
    {
      "epoch": 0.6996466431095406,
      "grad_norm": 1.5286486148834229,
      "learning_rate": 7.750275017224207e-05,
      "loss": 2.6259,
      "step": 99
    },
    {
      "epoch": 0.7067137809187279,
      "grad_norm": 1.5309984683990479,
      "learning_rate": 7.700954061750293e-05,
      "loss": 2.4393,
      "step": 100
    },
    {
      "epoch": 0.7137809187279152,
      "grad_norm": 1.7677701711654663,
      "learning_rate": 7.651259177050996e-05,
      "loss": 2.6965,
      "step": 101
    },
    {
      "epoch": 0.7208480565371025,
      "grad_norm": NaN,
      "learning_rate": 7.651259177050996e-05,
      "loss": 2.96,
      "step": 102
    },
    {
      "epoch": 0.7279151943462897,
      "grad_norm": 2.2894234657287598,
      "learning_rate": 7.60119724305441e-05,
      "loss": 3.1429,
      "step": 103
    },
    {
      "epoch": 0.734982332155477,
      "grad_norm": 2.8899526596069336,
      "learning_rate": 7.550775190504189e-05,
      "loss": 3.1791,
      "step": 104
    },
    {
      "epoch": 0.7420494699646644,
      "grad_norm": 4.414155960083008,
      "learning_rate": 7.500000000000001e-05,
      "loss": 3.2027,
      "step": 105
    },
    {
      "epoch": 0.7491166077738516,
      "grad_norm": 0.9304723739624023,
      "learning_rate": 7.448878701031142e-05,
      "loss": 1.8738,
      "step": 106
    },
    {
      "epoch": 0.7561837455830389,
      "grad_norm": 0.773618221282959,
      "learning_rate": 7.397418371003333e-05,
      "loss": 1.7726,
      "step": 107
    },
    {
      "epoch": 0.7632508833922261,
      "grad_norm": 0.8355372548103333,
      "learning_rate": 7.345626134258898e-05,
      "loss": 1.8941,
      "step": 108
    },
    {
      "epoch": 0.7703180212014135,
      "grad_norm": 0.8948445320129395,
      "learning_rate": 7.293509161090452e-05,
      "loss": 2.0912,
      "step": 109
    },
    {
      "epoch": 0.7773851590106007,
      "grad_norm": 0.8098867535591125,
      "learning_rate": 7.241074666748227e-05,
      "loss": 2.2609,
      "step": 110
    },
    {
      "epoch": 0.784452296819788,
      "grad_norm": 0.8708932399749756,
      "learning_rate": 7.188329910441154e-05,
      "loss": 2.354,
      "step": 111
    },
    {
      "epoch": 0.7915194346289752,
      "grad_norm": 1.0046172142028809,
      "learning_rate": 7.13528219433188e-05,
      "loss": 2.2907,
      "step": 112
    },
    {
      "epoch": 0.7985865724381626,
      "grad_norm": 0.8849496841430664,
      "learning_rate": 7.081938862525839e-05,
      "loss": 2.0898,
      "step": 113
    },
    {
      "epoch": 0.8056537102473498,
      "grad_norm": 1.120928168296814,
      "learning_rate": 7.028307300054499e-05,
      "loss": 2.322,
      "step": 114
    },
    {
      "epoch": 0.8127208480565371,
      "grad_norm": 0.93724524974823,
      "learning_rate": 6.974394931852956e-05,
      "loss": 2.5157,
      "step": 115
    },
    {
      "epoch": 0.8197879858657244,
      "grad_norm": 1.1139490604400635,
      "learning_rate": 6.920209221732006e-05,
      "loss": 2.3832,
      "step": 116
    },
    {
      "epoch": 0.8268551236749117,
      "grad_norm": 0.9670174717903137,
      "learning_rate": 6.865757671344827e-05,
      "loss": 1.9223,
      "step": 117
    },
    {
      "epoch": 0.833922261484099,
      "grad_norm": 1.027444839477539,
      "learning_rate": 6.811047819148413e-05,
      "loss": 2.3848,
      "step": 118
    },
    {
      "epoch": 0.8409893992932862,
      "grad_norm": 1.0481902360916138,
      "learning_rate": 6.756087239359947e-05,
      "loss": 2.3188,
      "step": 119
    },
    {
      "epoch": 0.8480565371024735,
      "grad_norm": 1.1003977060317993,
      "learning_rate": 6.700883540908184e-05,
      "loss": 2.6448,
      "step": 120
    },
    {
      "epoch": 0.8551236749116607,
      "grad_norm": 1.1545801162719727,
      "learning_rate": 6.64544436638005e-05,
      "loss": 1.7246,
      "step": 121
    },
    {
      "epoch": 0.8621908127208481,
      "grad_norm": 1.1439719200134277,
      "learning_rate": 6.589777390962575e-05,
      "loss": 2.4838,
      "step": 122
    },
    {
      "epoch": 0.8692579505300353,
      "grad_norm": 1.130408525466919,
      "learning_rate": 6.533890321380319e-05,
      "loss": 2.242,
      "step": 123
    },
    {
      "epoch": 0.8763250883392226,
      "grad_norm": 1.1255364418029785,
      "learning_rate": 6.477790894828421e-05,
      "loss": 2.6014,
      "step": 124
    },
    {
      "epoch": 0.8833922261484098,
      "grad_norm": 1.1765817403793335,
      "learning_rate": 6.421486877901437e-05,
      "loss": 2.4872,
      "step": 125
    },
    {
      "epoch": 0.8904593639575972,
      "grad_norm": 1.3128888607025146,
      "learning_rate": 6.364986065518106e-05,
      "loss": 2.2926,
      "step": 126
    },
    {
      "epoch": 0.8975265017667845,
      "grad_norm": 1.3941888809204102,
      "learning_rate": 6.308296279842205e-05,
      "loss": 3.008,
      "step": 127
    },
    {
      "epoch": 0.9045936395759717,
      "grad_norm": 1.3177193403244019,
      "learning_rate": 6.251425369199599e-05,
      "loss": 2.2648,
      "step": 128
    },
    {
      "epoch": 0.911660777385159,
      "grad_norm": 1.3153578042984009,
      "learning_rate": 6.194381206991722e-05,
      "loss": 2.3924,
      "step": 129
    },
    {
      "epoch": 0.9187279151943463,
      "grad_norm": 1.636897087097168,
      "learning_rate": 6.137171690605533e-05,
      "loss": 2.6597,
      "step": 130
    },
    {
      "epoch": 0.9257950530035336,
      "grad_norm": 1.5154238939285278,
      "learning_rate": 6.079804740320181e-05,
      "loss": 2.5498,
      "step": 131
    },
    {
      "epoch": 0.9328621908127208,
      "grad_norm": 1.5087813138961792,
      "learning_rate": 6.022288298210501e-05,
      "loss": 2.8785,
      "step": 132
    },
    {
      "epoch": 0.9399293286219081,
      "grad_norm": 1.4068495035171509,
      "learning_rate": 5.9646303270474845e-05,
      "loss": 2.5725,
      "step": 133
    },
    {
      "epoch": 0.9469964664310954,
      "grad_norm": 1.8009872436523438,
      "learning_rate": 5.9068388091958795e-05,
      "loss": 2.9385,
      "step": 134
    },
    {
      "epoch": 0.9540636042402827,
      "grad_norm": 1.6855831146240234,
      "learning_rate": 5.848921745509094e-05,
      "loss": 2.9886,
      "step": 135
    },
    {
      "epoch": 0.9611307420494699,
      "grad_norm": 1.928753137588501,
      "learning_rate": 5.79088715422152e-05,
      "loss": 3.0619,
      "step": 136
    },
    {
      "epoch": 0.9681978798586572,
      "grad_norm": 1.7998298406600952,
      "learning_rate": 5.7327430698384775e-05,
      "loss": 2.5613,
      "step": 137
    },
    {
      "epoch": 0.9752650176678446,
      "grad_norm": 2.052446126937866,
      "learning_rate": 5.6744975420238745e-05,
      "loss": 2.5973,
      "step": 138
    },
    {
      "epoch": 0.9823321554770318,
      "grad_norm": 2.038006544113159,
      "learning_rate": 5.616158634485793e-05,
      "loss": 2.7022,
      "step": 139
    },
    {
      "epoch": 0.9893992932862191,
      "grad_norm": 3.475146532058716,
      "learning_rate": 5.557734423860123e-05,
      "loss": 3.3906,
      "step": 140
    },
    {
      "epoch": 0.9964664310954063,
      "grad_norm": 1.2627288103103638,
      "learning_rate": 5.499232998592399e-05,
      "loss": 2.2368,
      "step": 141
    }
  ],
  "logging_steps": 1,
  "max_steps": 282,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 2,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 1832490864721920.0,
  "train_batch_size": 4,
  "trial_name": null,
  "trial_params": null
}
